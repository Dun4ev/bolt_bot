diff --git a/AGENTS.md b/AGENTS.md
index 988c631..cb95e57 100644
--- a/AGENTS.md
+++ b/AGENTS.md
@@ -30,6 +30,7 @@ pytest -q                        # минимальные регрессионн
 - По умолчанию `LLM_BACKEND=lm_studio` и запросы идут в локальный OpenAI-совместимый сервер LM Studio, адрес настраивайте через `LM_STUDIO_ENDPOINT`.
 - Чтобы использовать Google AI Studio (Gemini), пропишите в `.env` `LLM_BACKEND=gemini`, укажите `GEMINI_API_KEY` и нужную модель в `GEMINI_MODEL` (например, `gemini-1.5-flash`).
 - При backend=gemini ключ обязателен: отсутствие `GEMINI_API_KEY` приводит к ошибке запуска. Остальные переменные (Telegram токен, LM Studio параметры) остаются для локального режима и резервного использования.
+- В режиме работы бота пользователи могут вызвать `/backend` для выбора источника ответов (LM Studio или Gemini); выбор влияет на все последующие сообщения без перезапуска.
 
 ## Документация и версии — LITE
 **Источник правды по агентам:** один файл `AGENTS.md`. Если в проекте встречается `GEMINI.md` — перенеси релевантное и удали файл-двойник (единый контекст снижает риски рассинхронизации).
diff --git a/bolt_bot.py b/bolt_bot.py
index 2bb0068..e17e8aa 100644
--- a/bolt_bot.py
+++ b/bolt_bot.py
@@ -4,8 +4,15 @@ from pathlib import Path
 from typing import Any, Dict, List
 
 import requests
-from telegram import Update
-from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters
+from telegram import InlineKeyboardButton, InlineKeyboardMarkup, Update
+from telegram.ext import (
+    ApplicationBuilder,
+    CallbackQueryHandler,
+    CommandHandler,
+    ContextTypes,
+    MessageHandler,
+    filters,
+)
 
 DEFAULT_CHAT_MODEL = "gpt-oss-20b"
 DEFAULT_CARD_MODEL = "google/gemma-3n-e4b:2"
@@ -54,9 +61,13 @@ LLM_BACKEND = os.getenv("LLM_BACKEND", DEFAULT_BACKEND).lower()
 GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
 GEMINI_MODEL = os.getenv("GEMINI_MODEL", DEFAULT_GEMINI_MODEL)
 
-if LLM_BACKEND == "gemini" and not GEMINI_API_KEY:
+GEMINI_ENABLED = bool(GEMINI_API_KEY)
+
+if LLM_BACKEND == "gemini" and not GEMINI_ENABLED:
     raise RuntimeError("Для backend=gemini необходимо указать GEMINI_API_KEY в окружении")
 
+CURRENT_BACKEND = LLM_BACKEND
+
 # Функция для отправки промпта к выбранному backend
 def ask_lmstudio(prompt):
     messages = [
@@ -85,8 +96,9 @@ def call_chat_backend(
     max_tokens: int | None = None,
 ) -> str:
     """Отправляет запрос в выбранный LLM backend и возвращает текст ответа."""
+    backend = CURRENT_BACKEND
     target_model = model_name
-    if LLM_BACKEND == "gemini":
+    if backend == "gemini":
         target_model = GEMINI_MODEL or model_name
         return call_gemini_backend(messages, target_model, temperature, max_tokens)
     return call_lm_studio_backend(messages, target_model, temperature, max_tokens)
@@ -149,6 +161,46 @@ def call_gemini_backend(
         raise RuntimeError("Gemini API не вернул текстовый ответ")
     return parts[0].get("text", "")
 
+
+def backend_options_keyboard() -> InlineKeyboardMarkup:
+    buttons = [
+        InlineKeyboardButton("LM Studio (локально)", callback_data="backend:lm_studio")
+    ]
+    gemini_label = "Gemini (Google AI Studio)"
+    if not GEMINI_ENABLED:
+        gemini_label += " ❌"
+    buttons.append(InlineKeyboardButton(gemini_label, callback_data="backend:gemini"))
+    keyboard = InlineKeyboardMarkup.from_row(buttons)
+    return keyboard
+
+
+async def backend_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
+    await update.message.reply_text(
+        "Выберите источник ответов:", reply_markup=backend_options_keyboard()
+    )
+
+
+async def backend_callback(update: Update, context: ContextTypes.DEFAULT_TYPE):
+    query = update.callback_query
+    await query.answer()
+    _, backend = query.data.split(":", 1)
+    global CURRENT_BACKEND
+    if backend == "gemini" and not GEMINI_ENABLED:
+        await query.edit_message_text(
+            "Gemini API недоступен: укажите GEMINI_API_KEY в .env и перезапустите бота."
+        )
+        return
+    CURRENT_BACKEND = backend
+    await query.edit_message_text(
+        f"Backend переключён на {describe_backend(backend)}."
+    )
+
+
+def describe_backend(backend: str) -> str:
+    if backend == "gemini":
+        return "Gemini (Google AI Studio)"
+    return "LM Studio"
+
 async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
     await update.message.reply_text("Привет! Жду твой промпт.")
 
@@ -159,7 +211,10 @@ async def reply(update: Update, context: ContextTypes.DEFAULT_TYPE):
         answer = ask_lmstudio(question)
         await update.message.reply_text(answer)
     except Exception as e:
-        await update.message.reply_text(f"Ошибка при общении с LM Studio: {str(e)}")
+        backend_name = describe_backend(CURRENT_BACKEND)
+        await update.message.reply_text(
+            f"Ошибка при общении с {backend_name}: {str(e)}"
+        )
 
 if __name__ == '__main__':
     if not TELEGRAM_TOKEN:
@@ -167,5 +222,7 @@ if __name__ == '__main__':
     logging.basicConfig(level=logging.INFO)
     app = ApplicationBuilder().token(TELEGRAM_TOKEN).build()
     app.add_handler(CommandHandler("start", start))
+    app.add_handler(CommandHandler("backend", backend_command))
+    app.add_handler(CallbackQueryHandler(backend_callback, pattern=r"^backend:"))
     app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, reply))
     app.run_polling()
