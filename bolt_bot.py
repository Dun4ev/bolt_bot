import logging
import os
from pathlib import Path
from typing import Any, Dict, List

import time
import random
import requests
from telegram import InlineKeyboardButton, InlineKeyboardMarkup, ReplyKeyboardMarkup, Update
from telegram.ext import (
    ApplicationBuilder,
    CallbackQueryHandler,
    CommandHandler,
    ContextTypes,
    MessageHandler,
    filters,
)

DEFAULT_CHAT_MODEL = "gpt-oss-20b"
DEFAULT_CARD_MODEL = "google/gemma-3n-e4b:2"
DEFAULT_ENDPOINT = "http://127.0.0.1:1234/v1/chat/completions"
DEFAULT_BACKEND = "lm_studio"
DEFAULT_GEMINI_MODEL = "gemini-2.0-flash"
GEMINI_ENDPOINT_TEMPLATE = "https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent"
DEFAULT_GEMINI_TIMEOUT = 60
DEFAULT_LM_STUDIO_TIMEOUT = 30

POEM_PROMPT = """
–¢—ã ‚Äî —Ç–∞–ª–∞–Ω—Ç–ª–∏–≤—ã–π –ø–æ—ç—Ç, –ø–∏—à—É—â–∏–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ü–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É —Å–ª–æ–≤—É –∏–ª–∏ —Ñ—Ä–∞–∑–µ —Å–æ–∑–¥–∞–π –∫–æ—Ä–æ—Ç–∫–æ–µ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ –∏–∑ 4‚Äì8 —Å—Ç—Ä–æ–∫.

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –∏—Å–ø–æ–ª—å–∑—É–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–π —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –∏ —Ä–∏—Ç–º;
- –¥–æ–ø—É—Å–∫–∞–µ—Ç—Å—è –ª—é–±–∞—è —Ä–∏—Ñ–º–∞, –Ω–æ –∏–∑–±–µ–≥–∞–π –ø–æ–≤—Ç–æ—Ä–æ–≤;
- –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–ø–æ–º—è–Ω–∏ –∏—Å—Ö–æ–¥–Ω–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ —Ñ—Ä–∞–∑—É;
- –¥–æ–±–∞–≤—å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ—Ç—Ç–µ–Ω–æ–∫ (–ª–∏—Ä–∏—á–µ—Å–∫–∏–π –∏–ª–∏ –≤–¥–æ—Ö–Ω–æ–≤–ª—è—é—â–∏–π).

–í–æ—Ç –∏—Å—Ö–æ–¥–Ω–∞—è —Ç–µ–º–∞:
{product_text}
"""
POEM_HINT = "–ù–∞–ø–∏—à–∏—Ç–µ —Å–ª–æ–≤–æ –∏–ª–∏ —Ñ—Ä–∞–∑—É, –∏ —è —Å–æ—Å—Ç–∞–≤–ª—é —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ."
POEM_BUTTON = "üìù –°—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ"
BACKEND_BUTTON = "‚öôÔ∏è –í—ã–±—Ä–∞—Ç—å backend"


def load_env_file(env_file: str = ".env") -> None:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ .env, –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç."""
    env_path = Path(env_file)
    if not env_path.exists():
        return
    for raw_line in env_path.read_text(encoding="utf-8").splitlines():
        line = raw_line.strip()
        if not line or line.startswith("#") or "=" not in line:
            continue
        key, value = line.split("=", 1)
        key = key.strip()
        if not key or key in os.environ:
            continue
        value = value.strip().strip('"').strip("'")
        os.environ[key] = value


load_env_file()

TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
LM_STUDIO_ENDPOINT = os.getenv("LM_STUDIO_ENDPOINT", DEFAULT_ENDPOINT)
LM_STUDIO_MODEL = os.getenv("LM_STUDIO_MODEL", DEFAULT_CHAT_MODEL)
LM_STUDIO_CARD_MODEL = os.getenv("LM_STUDIO_CARD_MODEL", DEFAULT_CARD_MODEL)
LLM_BACKEND = os.getenv("LLM_BACKEND", DEFAULT_BACKEND).lower()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_MODEL = os.getenv("GEMINI_MODEL", DEFAULT_GEMINI_MODEL)

# Timeouts
GEMINI_TIMEOUT = int(os.getenv("GEMINI_TIMEOUT", DEFAULT_GEMINI_TIMEOUT))
LM_STUDIO_TIMEOUT = int(os.getenv("LM_STUDIO_TIMEOUT", DEFAULT_LM_STUDIO_TIMEOUT))

GEMINI_ENABLED = bool(GEMINI_API_KEY)

if LLM_BACKEND == "gemini" and not GEMINI_ENABLED:
    raise RuntimeError("–î–ª—è backend=gemini –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å GEMINI_API_KEY –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏")

CURRENT_BACKEND = LLM_BACKEND

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –ø—Ä–æ–º–ø—Ç–∞ –∫ –≤—ã–±—Ä–∞–Ω–Ω–æ–º—É backend
def ask_lmstudio(prompt):
    messages = [
        {"role": "system", "content": "–¢—ã helpful –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç."},
        {"role": "user", "content": prompt}
    ]
    return call_chat_backend(messages, LM_STUDIO_MODEL, temperature=0.7, max_tokens=1024)

# -----------------------------
# –ü—Ä–∏–º–µ—Ä —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–æ–≤–∞—Ä–Ω–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏ —Å –ø–æ–º–æ—â—å—é LLM

def ask_llm(product_text):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ –ø–æ –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–º–µ —Å –ø–æ–º–æ—â—å—é —à–∞–±–ª–æ–Ω–∞ POEM_PROMPT."""
    prompt = POEM_PROMPT.format(product_text=product_text)
    messages = [
        {"role": "system", "content": "–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª –ø–æ –Ω–∞–ø–∏—Å–∞–Ω–∏—é —Å—Ç–∏—Ö–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."},
        {"role": "user", "content": prompt}
    ]
    return call_chat_backend(messages, LM_STUDIO_CARD_MODEL, temperature=0.4)


def call_chat_backend(
    messages: List[Dict[str, str]],
    model_name: str,
    temperature: float = 0.7,
    max_tokens: int | None = None,
) -> str:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ –≤—ã–±—Ä–∞–Ω–Ω—ã–π LLM backend –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞."""
    backend = CURRENT_BACKEND
    target_model = model_name
    if backend == "gemini":
        target_model = GEMINI_MODEL or model_name
        return call_gemini_backend(messages, target_model, temperature, max_tokens)
    return call_lm_studio_backend(messages, target_model, temperature, max_tokens)


def call_lm_studio_backend(
    messages: List[Dict[str, str]],
    model_name: str,
    temperature: float,
    max_tokens: int | None,
) -> str:
    payload: Dict[str, Any] = {
        "model": model_name or LM_STUDIO_MODEL,
        "messages": messages,
        "temperature": temperature,
    }
    if max_tokens is not None:
        payload["max_tokens"] = max_tokens
    resp = requests.post(LM_STUDIO_ENDPOINT, json=payload, timeout=LM_STUDIO_TIMEOUT)
    resp.raise_for_status()
    content = resp.json()
    return content["choices"][0]["message"]["content"]


def call_gemini_backend(
    messages: List[Dict[str, str]],
    model_name: str,
    temperature: float,
    max_tokens: int | None,
) -> str:
    system_parts = [msg["content"] for msg in messages if msg.get("role") == "system"]
    conversation = [msg for msg in messages if msg.get("role") != "system"]
    payload: Dict[str, Any] = {
        "contents": [
            {
                "role": ("user" if msg.get("role") != "assistant" else "model"),
                "parts": [{"text": msg.get("content", "")}],
            }
            for msg in conversation
        ]
    }
    if system_parts:
        payload["systemInstruction"] = {
            "parts": [{"text": "\n\n".join(system_parts)}]
        }
    gen_config: Dict[str, Any] = {"temperature": temperature}
    if max_tokens is not None:
        gen_config["maxOutputTokens"] = max_tokens
    payload["generationConfig"] = gen_config
    endpoint = GEMINI_ENDPOINT_TEMPLATE.format(model=(model_name or GEMINI_MODEL))
    params = {"key": GEMINI_API_KEY}
    
    for attempt in range(1, 6):
        try:
            resp = requests.post(endpoint, params=params, json=payload, timeout=GEMINI_TIMEOUT)
            resp.raise_for_status()
            break
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:
                if attempt == 5:
                    raise
                sleep_time = (2 ** attempt) + (random.randint(0, 1000) / 1000)
                logging.warning(f"Gemini 429 Rate Limit. Retrying in {sleep_time:.2f}s (Attempt {attempt}/5)")
                time.sleep(sleep_time)
                continue
            raise

    content = resp.json()
    candidates = content.get("candidates", [])
    if not candidates:
        raise RuntimeError("Gemini API –Ω–µ –≤–µ—Ä–Ω—É–ª –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
    parts = candidates[0].get("content", {}).get("parts", [])
    if not parts:
        raise RuntimeError("Gemini API –Ω–µ –≤–µ—Ä–Ω—É–ª —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç")
    return parts[0].get("text", "")


def main_menu_keyboard() -> ReplyKeyboardMarkup:
    return ReplyKeyboardMarkup(
        [[POEM_BUTTON], [BACKEND_BUTTON]], resize_keyboard=True
    )


def backend_options_keyboard() -> InlineKeyboardMarkup:
    buttons = [
        InlineKeyboardButton("LM Studio (–ª–æ–∫–∞–ª—å–Ω–æ)", callback_data="backend:lm_studio")
    ]
    gemini_label = "Gemini (Google AI Studio)"
    if not GEMINI_ENABLED:
        gemini_label += " ‚ùå"
    buttons.append(InlineKeyboardButton(gemini_label, callback_data="backend:gemini"))
    keyboard = InlineKeyboardMarkup.from_row(buttons)
    return keyboard


async def backend_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "–í—ã–±–µ—Ä–∏—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫ –æ—Ç–≤–µ—Ç–æ–≤:", reply_markup=backend_options_keyboard()
    )


async def backend_callback(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    _, backend = query.data.split(":", 1)
    global CURRENT_BACKEND
    if backend == "gemini" and not GEMINI_ENABLED:
        await query.edit_message_text(
            "Gemini API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: —É–∫–∞–∂–∏—Ç–µ GEMINI_API_KEY –≤ .env –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –±–æ—Ç–∞."
        )
        return
    CURRENT_BACKEND = backend
    await query.edit_message_text(
        f"Backend –ø–µ—Ä–µ–∫–ª—é—á—ë–Ω –Ω–∞ {describe_backend(backend)}."
    )


def describe_backend(backend: str) -> str:
    if backend == "gemini":
        return "Gemini (Google AI Studio)"
    return "LM Studio"

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "–ü—Ä–∏–≤–µ—Ç! –ñ–¥—É —Ç–≤–æ–π –ø—Ä–æ–º–ø—Ç.", reply_markup=main_menu_keyboard()
    )


async def menu_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "–ú–µ–Ω—é –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π:", reply_markup=main_menu_keyboard()
    )

async def reply(update: Update, context: ContextTypes.DEFAULT_TYPE):
    question = (update.message.text or "").strip()
    if question == POEM_BUTTON:
        context.user_data["mode"] = "poem"
        await update.message.reply_text(POEM_HINT)
        return
    if question == BACKEND_BUTTON:
        await backend_command(update, context)
        return

    await update.message.reply_text("–î—É–º–∞—é...")
    
    try:
        if context.user_data.get("mode") == "poem":
            answer = ask_llm(question)
            # Reset mode after answering
            context.user_data["mode"] = None
        else:
            answer = ask_lmstudio(question)
        
        await update.message.reply_text(answer)
    except Exception as e:
        backend_name = describe_backend(CURRENT_BACKEND)
        await update.message.reply_text(
            f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—â–µ–Ω–∏–∏ —Å {backend_name}: {str(e)}"
        )

if __name__ == '__main__':
    if not TELEGRAM_TOKEN:
        raise RuntimeError("TELEGRAM_TOKEN –Ω–µ –∑–∞–¥–∞–Ω: –¥–æ–±–∞–≤—å—Ç–µ –µ–≥–æ –≤ .env –∏–ª–∏ –æ–∫—Ä—É–∂–µ–Ω–∏–µ")
    logging.basicConfig(level=logging.INFO)
    app = ApplicationBuilder().token(TELEGRAM_TOKEN).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("menu", menu_command))
    app.add_handler(CommandHandler("backend", backend_command))
    app.add_handler(CallbackQueryHandler(backend_callback, pattern=r"^backend:"))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, reply))
    app.run_polling()
